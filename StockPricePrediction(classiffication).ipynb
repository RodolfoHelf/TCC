{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing packages\n",
    "import random \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dropout,Dense\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the macroeconomics data \n",
    "\n",
    "def get_historical_dataset(interval):\n",
    "\n",
    "    #S&P NYSE NASDAQ\n",
    "    tickers = yf.Tickers(\"^GSPC ^NYA ^IXIC\")\n",
    "    NYA = tickers.tickers[\"^NYA\"].history(start=\"2014-09-15\", end=\"2022-09-07\",interval=interval)\n",
    "    NYA = NYA[[\"Open\",\"High\",\"Close\",\"Low\",\"Volume\"]]\n",
    "    NYA = NYA.rename({'Open': 'Open1', 'High': 'High1','Close': 'Close1','Low': 'Low1','Volume': 'Volume1'}, axis=1)\n",
    "    SEP = tickers.tickers[\"^GSPC\"].history(start=\"2014-09-15\", end=\"2022-09-07\",interval=interval)\n",
    "    SEP = SEP[[\"Open\",\"High\",\"Close\",\"Low\",\"Volume\"]]\n",
    "    SEP = SEP.rename({'Open': 'Open2', 'High': 'High2','Close': 'Close2','Low': 'Low2','Volume': 'Volume2'}, axis=1)\n",
    "    NASDAQ = tickers.tickers[\"^IXIC\"].history(start=\"2014-09-15\", end=\"2022-09-07\",interval=interval)\n",
    "    NASDAQ = NASDAQ[[\"Open\",\"High\",\"Close\",\"Low\",\"Volume\"]]\n",
    "    NASDAQ = NASDAQ.rename({'Open': 'Open3', 'High': 'High3','Close': 'Close3','Low': 'Low3','Volume': 'Volume3'}, axis=1) \n",
    "\n",
    "    #loading the BTC data\n",
    "    BTC_Ticker = yf.Ticker(\"BTC-USD\")\n",
    "    BTC_Data = BTC_Ticker.history(start=\"2014-09-15\", end=\"2022-09-07\",interval = interval)\n",
    "    BTC_Data = BTC_Data[[\"Open\",\"High\",\"Close\",\"Low\",\"Volume\"]]\n",
    "    new_dataset = pd.concat([BTC_Data,NYA,SEP,NASDAQ],axis=1)\n",
    "    return new_dataset\n",
    "\n",
    "new_dataset = get_historical_dataset(\"1wk\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BTC</th>\n",
       "      <th>NYA</th>\n",
       "      <th>SPX</th>\n",
       "      <th>NASDAQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BTC</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905751</td>\n",
       "      <td>0.910254</td>\n",
       "      <td>0.918160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NYA</th>\n",
       "      <td>0.905751</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972339</td>\n",
       "      <td>0.945198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPX</th>\n",
       "      <td>0.910254</td>\n",
       "      <td>0.972339</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NASDAQ</th>\n",
       "      <td>0.918160</td>\n",
       "      <td>0.945198</td>\n",
       "      <td>0.988888</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             BTC       NYA       SPX    NASDAQ\n",
       "BTC     1.000000  0.905751  0.910254  0.918160\n",
       "NYA     0.905751  1.000000  0.972339  0.945198\n",
       "SPX     0.910254  0.972339  1.000000  0.988888\n",
       "NASDAQ  0.918160  0.945198  0.988888  1.000000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation = new_dataset[[\"Close\",\"Close1\",\"Close2\",\"Close3\"]]\n",
    "correlation= correlation.rename({'Close': 'BTC', 'Close1': 'NYA','Close2': 'SPX','Close3': 'NASDAQ'}, axis=1) \n",
    "correlation.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(417, 20)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the close values as a vertical array ( add one more dimension to the array)\n",
    "#features_array =np.concatenate( (new_dataset.Close.values.reshape(-1,1), new_dataset.Open.values.reshape(-1,1),new_dataset.High.values.reshape(-1,1),new_dataset.Low.values.reshape(-1,1)),axis=1)\n",
    "features_array = new_dataset.to_numpy()\n",
    "#close_values=dataframe_close.reshape(-1,1)\n",
    "#close_values\n",
    "features_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.65864014e+02, 4.68174011e+02, 3.98821014e+02, 3.84532013e+02,\n",
       "       1.56903400e+08, 1.09098496e+04, 1.10605996e+04, 1.09895703e+04,\n",
       "       1.08810498e+04, 1.72618200e+10, 1.98604004e+03, 2.01926001e+03,\n",
       "       2.01040002e+03, 1.97847998e+03, 1.72618200e+10, 4.56745020e+03,\n",
       "       4.61056982e+03, 4.57979004e+03, 4.49987012e+03, 1.05699100e+10])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split the values in train and test\n",
    "dataset_train = np.array(features_array[:int(features_array.shape[0]*0.8)])\n",
    "dataset_test = np.array(features_array[int(features_array.shape[0]*0.8):])\n",
    "dataset_train[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29891985, 0.4254457 , 0.42474225, ..., 0.58145986, 0.49247005,\n",
       "        0.77543144],\n",
       "       [0.42379553, 0.60312038, 0.63727123, ..., 0.62693728, 0.62923905,\n",
       "        1.        ],\n",
       "       [0.63601672, 0.78494514, 0.82833494, ..., 0.58491295, 0.60408734,\n",
       "        0.58484864],\n",
       "       ...,\n",
       "       [0.04833311, 0.0339192 , 0.00707048, ..., 0.25543585, 0.30238414,\n",
       "        0.4033668 ],\n",
       "       [0.00687608, 0.00796505, 0.01508101, ..., 0.15829937, 0.18825012,\n",
       "        0.45389325],\n",
       "       [0.01495922, 0.        , 0.        , ..., 0.1419562 , 0.17385974,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalize the data \n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "dataset_train = scaler.fit_transform(dataset_train)\n",
    "dataset_test = scaler.fit_transform(dataset_test)\n",
    "\n",
    "\n",
    "\n",
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset with the last (timesteps) prices (x) as with the next movement (y, 1 if price up and 0 if price down)\n",
    "timesteps = 10\n",
    " \n",
    "def create_dataset2(df):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(timesteps, df.shape[0]):\n",
    "        x.append(df[i-timesteps:i])\n",
    "        if(df[i,0]>df[i-1,0]):\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 39 (74,)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = create_dataset2(dataset_train)\n",
    "x_test, y_test = create_dataset2(dataset_test)\n",
    "\n",
    "count1=0\n",
    "for i in y_test:\n",
    "    if i == 1:\n",
    "        count1+=1\n",
    "\n",
    "print(count1,len(y_test)-count1,y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a 3D array to use in LTSM layer\n",
    "#Tensors are the data structure used by machine learning systems, and getting to know them is an essential skill you should build early on.\n",
    "#LTSM receive 3D tensor\n",
    "\n",
    "#A tensor is a container for numerical data. It is the way we store the information that we’ll use within our system. [samples, time steps, features]\n",
    "#The batch size is the size of the training batch you use. It can be anything and doesn't effect the size of the LSTM layer. It just modifies the gradient update step.\n",
    "\n",
    "#Time steps is the one that determines the size, because it's the number of times that you unroll your LSTM cell. So, that is right, total number of unrolled cells is equal to 5.\n",
    "\n",
    "#The features is related to the series you want to input/predict. If it is 1, the series is univariate, otherwise it is multi-variate and doesn't have anything to do with the number of cells unrolled.\n",
    "\n",
    "\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], x_train.shape[2]))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], x_train.shape[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create the sequence model\n",
    "def create_model(units,dropout,outputs,features):\n",
    "    model = Sequential()\n",
    "#we initialized our model as a sequential one with 96 units in the output’s dimensionality. We used return_sequences=True to make the LSTM layer with three-dimensional input and input_shape to shape our dataset.\n",
    "#inputs: A 3D tensor with shape [batch, timesteps, feature]\n",
    "# units : dimensionality of the output space .I think that more units (greater dimension of hidden states) will help the network to remember more complex patterns.The main point is that there is usually no rule for the number of hidden nodes you should use, \n",
    "# it is something you have to figure out for each case by trial and error.\n",
    "#return sequences : Whether to return the last output. in the output sequence, or the full sequence. Default: False .This allows us to have 3D output from hidden LSTM layer as input to the next.\n",
    "#stateful:  If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch.\n",
    "#input_shape = [n_steps,n_features]\n",
    "    model.add(LSTM(units=units,return_sequences=True, input_shape=(timesteps, features)))\n",
    "#The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting.\n",
    "#  Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged. Making the dropout fraction 0.2 drops 20% of the layers\n",
    "# reduce overfitting and improve generalization error in deep neural networks of all kinds.\n",
    "#One approach to reduce overfitting is to fit all possible different neural networks on the same dataset and to average the predictions from each model. \n",
    "# This is not feasible in practice, and can be approximated using a small collection of different models, called an ensemble\n",
    "# During training, some number of layer outputs are randomly ignored or “dropped out.” \n",
    "# This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer. \n",
    "# In effect, each update to a layer during training is performed with a different “view” of the configured layer.\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(LSTM(units=units,return_sequences=True))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(LSTM(units=units,return_sequences=True))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(LSTM(units=units))\n",
    "#Finally, we added a dense layer with a value of 1 because we want to output one value.\n",
    "#The dense layer’s neuron in a model receives output from every neuron of its preceding layer, where neurons of the dense layer perform matrix-vector multiplication.\n",
    "#dense layer is basically used for changing the dimensions of the vector.\n",
    "#model.add(Dense(units=units,activation=\"relu\"))\n",
    "    model.add(Dense(units=outputs,activation=\"softmax\"))\n",
    "    model.summary()\n",
    "    return model\n",
    "#num_params = g × [h(h+i) + h]\n",
    "#g, no. of FFNNs in a unit (RNN has 1, GRU has 3, LSTM has 4)\n",
    "#h, size of hidden units (units)\n",
    "#i, dimension/size of input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We used loss='mean_squared_error' because it is a regression problem, and the adam optimizer to update network weights iteratively based on training data.\n",
    "#The purpose of loss functions is to compute the quantity that a model should seek to minimize during training.\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 10, 100)           48400     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 10, 100)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 10, 100)           80400     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10, 100)           0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 10, 100)           80400     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 10, 100)           0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 202       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 289,802\n",
      "Trainable params: 289,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 6s 28ms/step - loss: 0.6863 - accuracy: 0.5449\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6890 - accuracy: 0.5820\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6842 - accuracy: 0.5820\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6798 - accuracy: 0.5820\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.6810 - accuracy: 0.5820\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6801 - accuracy: 0.5820\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6811 - accuracy: 0.5820\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6825 - accuracy: 0.5697\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6862 - accuracy: 0.5759\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6911 - accuracy: 0.5511\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6868 - accuracy: 0.5542\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6756 - accuracy: 0.5820\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6843 - accuracy: 0.5820\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6843 - accuracy: 0.5820\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6798 - accuracy: 0.5820\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6819 - accuracy: 0.5820\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6823 - accuracy: 0.5820\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6821 - accuracy: 0.5820\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6816 - accuracy: 0.5820\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6808 - accuracy: 0.5820\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.6787 - accuracy: 0.5820\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6788 - accuracy: 0.5820\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6812 - accuracy: 0.5820\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6843 - accuracy: 0.5820\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6828 - accuracy: 0.5820\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6786 - accuracy: 0.5820\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6790 - accuracy: 0.5820\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6791 - accuracy: 0.5820\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6789 - accuracy: 0.5820\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6791 - accuracy: 0.5820\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.6782 - accuracy: 0.5820\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6771 - accuracy: 0.5820\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.6774 - accuracy: 0.5975\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6808 - accuracy: 0.5913\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6821 - accuracy: 0.5697\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6802 - accuracy: 0.5882\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6783 - accuracy: 0.5820\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6766 - accuracy: 0.5820\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6751 - accuracy: 0.5851\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6730 - accuracy: 0.5913\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 35ms/step - loss: 0.6734 - accuracy: 0.6068\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6745 - accuracy: 0.5913\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6742 - accuracy: 0.6068\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6775 - accuracy: 0.5975\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6773 - accuracy: 0.5851\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6767 - accuracy: 0.5851\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.6751 - accuracy: 0.5882\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.6749 - accuracy: 0.5820\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6751 - accuracy: 0.5882\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.6735 - accuracy: 0.5820\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.6718 - accuracy: 0.6130\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6860 - accuracy: 0.5573\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6709 - accuracy: 0.6037\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6732 - accuracy: 0.5913\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6749 - accuracy: 0.5851\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.6757 - accuracy: 0.5820\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.6761 - accuracy: 0.5820\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6757 - accuracy: 0.5820\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6757 - accuracy: 0.5820\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6763 - accuracy: 0.5913\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6758 - accuracy: 0.6037\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.6728 - accuracy: 0.5944\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.6748 - accuracy: 0.6006\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6795 - accuracy: 0.5882\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6718 - accuracy: 0.6006\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6773 - accuracy: 0.5882\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6707 - accuracy: 0.6037\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6700 - accuracy: 0.6099\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6700 - accuracy: 0.6161\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6693 - accuracy: 0.6285\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6714 - accuracy: 0.6006\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6717 - accuracy: 0.5820\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6755 - accuracy: 0.5851\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6769 - accuracy: 0.5851\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6802 - accuracy: 0.5820\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6796 - accuracy: 0.5820\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6762 - accuracy: 0.5789\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6753 - accuracy: 0.5882\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6745 - accuracy: 0.5820\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6750 - accuracy: 0.5820\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.6765 - accuracy: 0.5820\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6718 - accuracy: 0.5913\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 0.6757 - accuracy: 0.6223\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6841 - accuracy: 0.5820\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6841 - accuracy: 0.5542\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6785 - accuracy: 0.5944\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6768 - accuracy: 0.6037\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 0.6747 - accuracy: 0.5975\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6734 - accuracy: 0.6037\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6721 - accuracy: 0.6006\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6747 - accuracy: 0.6006\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6704 - accuracy: 0.6161\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6703 - accuracy: 0.6006\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6750 - accuracy: 0.5913\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6720 - accuracy: 0.6006\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6708 - accuracy: 0.5975\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6732 - accuracy: 0.6130\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.6751 - accuracy: 0.6068\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 0.6733 - accuracy: 0.5975\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 0.6720 - accuracy: 0.6099\n",
      "2/2 [==============================] - 2s 6ms/step - loss: 0.7516 - accuracy: 0.5270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5270270109176636]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#epochs: Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided\n",
    "#batch_size: Number of samples per gradient update.\n",
    "\n",
    "#variables for test\n",
    "units = 100\n",
    "dropout = 0\n",
    "outputs = y_test.shape[1]\n",
    "features = x_train.shape[2]\n",
    "\n",
    "accuracy_list = []\n",
    "for _ in range(1):\n",
    "    model = create_model(units,dropout,outputs,features)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=100, batch_size=64)\n",
    "\n",
    "    loss,accuracy = model.evaluate(x_test, y_test, batch_size=64)\n",
    "    accuracy_list.append(accuracy)\n",
    "accuracy_list\n",
    "\n",
    "#model.get_config()[\"layers\"][1][\"config\"]\n",
    "#model.save('stock_prediction.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the signals from the predictions\n",
    "#TODO test the model varying the timesteps,batch_size,epochs,number of features,number of units,dropout\n",
    "def create_list_signals2(predictions) :\n",
    "    buy_sell_signal = []\n",
    "    for index in range(len(predictions)-1):\n",
    "        if (abs(predictions[index][0] - predictions[index][1]))<0.1:\n",
    "            buy_sell_signal.append(0)\n",
    "        else:\n",
    "            if predictions[index][0] > predictions[index][1]:\n",
    "                buy_sell_signal.append(-1)\n",
    "            if predictions[index][1] > predictions[index][0]:\n",
    "                buy_sell_signal.append(1)\n",
    "\n",
    "    return buy_sell_signal\n",
    "\n",
    "def create_list_signals(predictions) :\n",
    "    buy_sell_signal = []\n",
    "    for index in range(len(predictions)-1):\n",
    "        if predictions[index][0] > predictions[index][1]:\n",
    "            buy_sell_signal.append(-1)\n",
    "        if predictions[index][1] > predictions[index][0]:\n",
    "            buy_sell_signal.append(1)\n",
    "    return buy_sell_signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating signals from other strategies\n",
    "\n",
    "def create_random_signals(predictions):\n",
    "    random_signal = []\n",
    "    for i in range(len(predictions)-1):\n",
    "        random_signal.append(random.randint(0,1))\n",
    "    return random_signal\n",
    "\n",
    "\n",
    "def create_random_signals(predictions):\n",
    "    test_dataset = new_dataset[-len(predictions):].reset_index()[[\"Open\",\"Close\"]]\n",
    "    simple_signal = []\n",
    "    for index in range(1,len(predictions)):\n",
    "        if test_dataset.Close[index-1] > test_dataset.Open[index-1]:\n",
    "            simple_signal.append(1)\n",
    "        else:\n",
    "            simple_signal.append(0)\n",
    "    return simple_signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the return of investimenting using the signals\n",
    "def test_signals(buy_sell_signal):\n",
    "    capital = 100\n",
    "    capital_list = []\n",
    "    test_dataset = new_dataset[-len(predictions):].reset_index()[[\"Open\",\"Close\"]]\n",
    "\n",
    "    for index,signal in enumerate(buy_sell_signal):\n",
    "        if signal == 1:\n",
    "            capital = capital/test_dataset.Open[index+1] * test_dataset.Close[index+1]\n",
    "            capital_list.append(capital)\n",
    "        elif signal == -1 :\n",
    "            capital = capital/test_dataset.Close[index+1] * test_dataset.Open[index+1]\n",
    "            capital_list.append(capital)\n",
    "        else:\n",
    "            capital = capital\n",
    "            capital_list.append(capital)\n",
    "        \n",
    "        \n",
    "    return capital_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = model.predict(x_test)\n",
    "\n",
    "capital_list = test_signals(create_list_signals(predictions))\n",
    "plt.plot(capital_list,'r',label = \"Model Signals\")\n",
    "capital_list = test_signals(create_list_signals2(predictions))\n",
    "plt.plot(capital_list,'b',label = \"Model Signals2\")\n",
    "#capital_list = test_signals(random_signal)\n",
    "#plt.plot(capital_list,'g',label = \"Random Signal\")\n",
    "#capital_list = test_signals(simple_signal)\n",
    "#plt.plot(capital_list,'b',label = \"Simple Signal\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.metrics import mean_absolute_error as mae\\nfrom sklearn.metrics import mean_absolute_percentage_error as mape\\nfrom sklearn.metrics import mean_squared_error as mse\\n#calculation of the error metrics in predictions\\nmae = mae(y_test_scaled,predictions)\\nmape = mape(y_test_scaled,predictions)\\nmse = mse(y_test_scaled,predictions)\\nprint(mae,mape,mse)'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "#calculation of the error metrics in predictions\n",
    "mae = mae(y_test_scaled,predictions)\n",
    "mape = mape(y_test_scaled,predictions)\n",
    "mse = mse(y_test_scaled,predictions)\n",
    "print(mae,mape,mse)'''"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b7e9cb8e453d6cda0fe8c8dd13f891a1f09162f0e7c66ffeae7751a7aecf00d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
